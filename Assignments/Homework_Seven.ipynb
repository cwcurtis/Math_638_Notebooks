{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "\n",
    "**Problem 1**) (10 pts) Using the code for the two-stage RK scheme provided in the notes, implement the 4-stage RK scheme using the method described in the textbook. (5pts)  Test your method on the problem\n",
    "\n",
    "$$\n",
    "\\dot{x} = x(1-x), ~ x(0) = \\frac{1}{2}.\n",
    "$$\n",
    "\n",
    "Do you get an error that goes like $(\\delta t)^{4}$? (5pts)  Note, to answer this, pick a reference time, like $t_{f}=2$.  Derive an exact solution, $x(t)$, and then compare $x(t_{f})$ to $x^{(RK)}(t_{f})$ where $x^{(RF)}(t_{f})$ is the approximation generated by the RK scheme.  If our theory is correct, then we should have \n",
    "\n",
    "$$\n",
    "\\left|x(t_{f}) -  x^{(RK)}(t_{f})\\right| \\approx C (\\delta t)^4\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "\\log_{10}\\left|x(t_{f}) -  x^{(RK)}(t_{f})\\right| \\approx \\log_{10}C  + 4 \\log_{10}(\\delta t)\n",
    "$$\n",
    "\n",
    "so that if we made an array of $\\delta t$ values, like \n",
    "\n",
    "`\n",
    "dtvals = np.array([1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4])\n",
    "`\n",
    "\n",
    "and then ran the RK scheme over each time step and kept track of the error in an array, and then plotted the array against the `dtvals` on a log/log plot, we should see a line of slope 4 appear.  See also problem 2.8.3/2.8.4 for further reference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2**) (5 pts) 7.3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3**) (5 pts) 7.6.14.  Note by \"averaged\", just use the method of multiple scales outlined in the notes and class.  As for the numerics, see if you can adapt the RK-4 method to two dimensional systems.  See Section 6.1 of the book for help.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**) (5 pts) Let $A$ be a real $n\\times n$ square matrix so that $A:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$.  Moreover, suppose that $A$ is diagonalizable so that $A = V\\Lambda V^{-1}$, where $\\Lambda$ is a diagonal matrix.  Show that the initial value problem \n",
    "\n",
    "$$\n",
    "\\dot{{\\bf x}} = A{\\bf x} + {\\bf g}(t), ~ {\\bf x}(0)={\\bf x}_{0}, \n",
    "$$\n",
    "\n",
    "is solved via Duhamel's formula\n",
    "\n",
    "$$\n",
    "{\\bf x}(t) = e^{At}{\\bf x}_{0} + e^{At}\\int_{0}^{t}e^{-As}{\\bf g}(s)ds.   \n",
    "$$\n",
    "\n",
    "To do this, you will first need to show that for a diagonlizable matrix one has \n",
    "\n",
    "$$\n",
    "e^{At} = \\sum_{j=0}^{\\infty}\\frac{A^{j}t^{j}}{j!} = V\\left(\\sum_{j=0}^{\\infty}\\frac{\\Lambda^{j}t^{j}}{j!}\\right)V^{-1} = Ve^{\\Lambda t}V^{-1}.\n",
    "$$\n",
    "\n",
    "Then you need to show that if the diagonal matrix $\\Lambda$ is given by \n",
    "\n",
    "$$\n",
    "\\Lambda = \\begin{pmatrix}\\lambda_{1} & & & \\\\ & \\lambda_{2} &  &\\\\ & & \\ddots & \\\\ & & & \\lambda_{n} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "then \n",
    "\n",
    "$$\n",
    "e^{\\Lambda t} = \\begin{pmatrix} e^{\\lambda_{1}t} & & & \\\\ & e^{\\lambda_{2}t} &  &\\\\ & & \\ddots & \\\\ & & & e^{\\lambda_{n}t} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Finally, show \n",
    "\n",
    "$$\n",
    "\\frac{d}{dt} e^{At} = Ae^{At} = e^{At}A. \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**) (10 pts) Let $A$ be a real $n\\times n$ square matrix so that $A:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$.  Moreover, suppose that $A=-A^{T}$, which ensures that $A$ is diagonalizable so that $A = V\\Lambda V^{-1}$, where $\\Lambda$ is a diagonal matrix.  Moreover, it ensures each eigenvalue $\\lambda$ is on the imaginary axis, i.e. $\\lambda = i\\tilde{\\lambda}, ~ \\tilde{\\lambda}\\in \\mathbb{R}$.  Suppose we have the dynamical system \n",
    "\n",
    "$$\n",
    "\\dot{{\\bf x}} = A{\\bf x} + \\epsilon f\\left({\\bf x} \\right), ~ {\\bf x}(0)={\\bf x}_{0}, ~ 0\\leq \\epsilon \\ll 1,\n",
    "$$\n",
    "\n",
    "where $f\\left({\\bf x}\\right)$ represents the nonlinear terms and $f(0)=0$.  \n",
    "\n",
    "* Using a regular-perturbation expansion of the form \n",
    "$$\n",
    "{\\bf x}(t) = \\tilde{{\\bf x}}_{0}(t) + \\epsilon \\tilde{{\\bf x}}_{1}(t) + \\mathcal{O}(\\epsilon^{2})\n",
    "$$\n",
    "Show that the first two equations that come from this are \n",
    "$$\n",
    "\\frac{d\\tilde{{\\bf x}}_{0}}{dt} = A\\tilde{{\\bf x}}_{0}, ~ \\tilde{{\\bf x}}_{0}(0) = {\\bf x}_{0}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{d\\tilde{{\\bf x}}_{1}}{dt} = A\\tilde{{\\bf x}}_{1} + f\\left(\\tilde{{\\bf x}}_{0}(t)\\right), ~ \\tilde{{\\bf x}}_{1}(0) = 0\n",
    "$$\n",
    "Using Duhamel's formula then, show that \n",
    "$$\n",
    "\\tilde{{\\bf x}}_{1}(t) = e^{At}\\int_{0}^{t}e^{-As}f\\left(e^{As} {\\bf x}_{0}\\right)ds.\n",
    "$$\n",
    "\n",
    "* How would one determine whether or not a given nonlinear system exhibits resonance?  So, to get our heads around this, we need to think about more efficient ways to represent multi-dimensional Taylor series expansions.  This is done in the following way.  Let an n-dimensional multi-index $\\vec{\\alpha}$ be defined so that \n",
    "$$\n",
    "\\vec{\\alpha} = \\left(\\alpha_{1},\\cdots,\\alpha_{n}\\right), ~ \\alpha_{j}\\in \\mathbb{N}\\cup\\left\\{0\\right\\}\n",
    "$$\n",
    "and let $\\left|\\vec{\\alpha}\\right|$ be defined so that \n",
    "$$\n",
    "\\left|\\vec{\\alpha}\\right| = \\sum_{l=1}^{n} \\alpha_{l}.\n",
    "$$\n",
    "For the $n$-vector ${\\bf x}=(x_{1},x_{2},\\cdots,x_{n})$, by the symbol ${\\bf x}^{\\vec{\\alpha}}$ we mean\n",
    "$$\n",
    "{\\bf x}^{\\vec{\\alpha}} = x_{1}^{\\alpha_{1}}x_{2}^{\\alpha_{2}}\\cdots x_{n}^{\\alpha_{n}},\n",
    "$$\n",
    "and by the symbol $\\partial^{\\vec{\\alpha}}_{{\\bf x}}f_{l}$ we mean \n",
    "$$\n",
    "\\partial^{\\vec{\\alpha}}_{{\\bf x}}f_{l} = \\partial_{x_{1}}^{\\alpha_{1}}\\partial_{x_{2}}^{\\alpha_{2}}\\cdots\\partial_{x_{n}}^{\\alpha_{n}}f_{l}.\n",
    "$$\n",
    "Thus, when we say that $f:\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{n}$, so that $f({\\bf x})=\\left(f_{1}({\\bf x}),f_{2}({\\bf x}), \\cdots, f_{n}({\\bf x})\\right)$, represents the _nonlinearity_ in our problem, we are tacitly saying that each coordinate of the map $f$, i.e. $f_{l}({\\bf x})$, can be Taylor expanded around the origin so that  \n",
    "$$\n",
    "f_{l}({\\bf x}) = \\sum_{\\left|\\vec{\\alpha}\\right|=j, j\\geq 2}\\frac{{\\bf x}^{\\vec{\\alpha}}}{\\vec{\\alpha}!}\\partial^{\\vec{\\alpha}}_{{\\bf x}}f_{l}\\left(0\\right), ~ \\vec{\\alpha}! = \\Pi_{m=1}^{n}\\left(\\alpha_{m}!\\right)\n",
    "$$\n",
    "where when we write $\\sum_{\\left|\\vec{\\alpha}\\right|=j, j\\geq 2}$ we mean that we sum over all possible multi-indices $\\vec{\\alpha}$ such that $\\left|\\vec{\\alpha}\\right|=j$ and $j\\geq 2$. Then you should use everything from problem 4 above to help you identify resonant conditions.  (More hints to follow).\n",
    "\n",
    "* How would one build a multiple scales expansion to remove resonances?  (Hints to follow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6**) (10 pts) For two-dimensional reversible systems, or at least those that are reversible under the $(x,y)\\rightarrow (x,-y)$ transformation, whose linearization are given by a harmonic oscillator, we showed that a polar coordinates transformation leads us to the leading order system\n",
    "\n",
    "\\begin{align*}\n",
    "\\dot{r} = & r^{2}f(\\theta)\\\\\n",
    "\\dot{\\theta} = & -1 + rg(\\theta), \n",
    "\\end{align*}\n",
    "\n",
    "where we have the symmetries\n",
    "\n",
    "$$\n",
    "f(-\\theta) = -f(\\theta), ~ g(-\\theta) = g(\\theta).\n",
    "$$\n",
    "\n",
    "The periodicity of $f$ and $g$ imply that we have the Fourier series representations \n",
    "\n",
    "$$\n",
    "f(\\theta) = \\sum_{m=-\\infty}^{\\infty}\\hat{f}_{m}e^{im\\theta}, ~~ g(\\theta) = \\sum_{m=-\\infty}^{\\infty}\\hat{g}_{m}e^{im\\theta}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\hat{f}_{m} = \\frac{1}{2\\pi}\\int_{0}^{2\\pi}e^{-im\\theta}f(\\theta)d\\theta, ~~ \\hat{g}_{m} = \\frac{1}{2\\pi}\\int_{0}^{2\\pi}e^{-im\\theta}g(\\theta)d\\theta\n",
    "$$\n",
    "\n",
    "and where we assume both $f$ and $g$ are real functions so that $\\hat{f}_{-m} = \\hat{f}^{\\ast}_{m}$ and $\\hat{g}_{-m} = \\hat{g}^{\\ast}_{m}$ where $\\ast$ denotes complex-conjugation.  \n",
    "\n",
    "* Show that the odd symmetry in $f$ implies that $\\hat{f}_{0}=0$.\n",
    "* Show that \n",
    "$$\n",
    "r(t) = \\frac{r_{0}}{1-r_{0}\\int_{0}^{t}f(\\theta(s))ds}\n",
    "$$\n",
    "* Show that if you let $\\theta(t) = -t + \\theta_{0} + r_{0}\\tilde{\\theta}$, then one has \n",
    "$$\n",
    "\\dot{\\tilde{\\theta}} = \\frac{g\\left(-t+\\theta_{0}+r_{0}\\tilde{\\theta}(t)\\right)}{1-r_{0}\\int_{0}^{t}f(-s-\\theta_{0}+r_{0}\\tilde{\\theta}(s))ds}\n",
    "$$\n",
    "* If we start on the x-axis, so that $\\theta_{0}=0$, using series arguments, how small should you choose your initial radius $r(0)=r_{0}$ to ensure that $r(t)$ remains bounded up to the point it strikes the other side of the x-axis, i.e. when $\\theta(t)=\\pi$?  To provide a reasonable, though albeit hand-wavy, argument, you will need to think about the dynamics associated with the approximation\n",
    "$$\n",
    "\\dot{\\tilde{\\theta}} \\sim g\\left(-t \\right)\n",
    "$$\n",
    "and what that means about the behavior of the integral \n",
    "$$\n",
    "\\int_{0}^{t}f\\left(-s - r_{0}\\tilde{\\theta}(s) \\right) ds \\sim \\int_{0}^{t}f\\left(-(1 + r_{0}\\hat{g}_{0})s - r_{0}G(s) \\right) ds\n",
    "$$\n",
    "where\n",
    "$$\n",
    "G(t) = -\\sum_{m\\neq 0}\\frac{\\hat{g}_{m}}{im}\\left(e^{-imt} -1 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
